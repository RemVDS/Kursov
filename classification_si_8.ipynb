{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f0dbd82f",
   "metadata": {},
   "source": [
    "# Классификация: SI > 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f69f64b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [01:33:35] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 273, number of negative: 499\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.004619 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 18123\n",
      "[LightGBM] [Info] Number of data points in the train set: 772, number of used features: 170\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.353627 -> initscore=-0.603134\n",
      "[LightGBM] [Info] Start training from score -0.603134\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "Logistic Regression: Accuracy=0.711, Precision=0.600, Recall=0.565, F1=0.582, AUC=0.703\n",
      "Random Forest: Accuracy=0.691, Precision=0.579, Recall=0.478, F1=0.524, AUC=0.731\n",
      "XGBoost: Accuracy=0.696, Precision=0.566, Recall=0.623, F1=0.593, AUC=0.735\n",
      "LightGBM: Accuracy=0.675, Precision=0.548, Recall=0.493, F1=0.519, AUC=0.736\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "\n",
    "# Загрузка данных\n",
    "df = pd.read_csv('/Users/rem/МИФИ/курсовая/dataset.csv')\n",
    "\n",
    "\n",
    "features = [col for col in df.columns if col not in [\"IC50, mM\", \"CC50, mM\", \"SI\"]]\n",
    "# Подготовка целевой переменной\n",
    "y = (df[\"SI\"] > 8).astype(int)\n",
    "X = df[features]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y, test_size=0.2, random_state=42)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Для моделей на основе градиентного бустинга (LightGBM) необходимо очистить имена признаков от специальных символов\n",
    "def clean_column(name):\n",
    "    return re.sub(r'[^\\w]', '_', name)\n",
    "\n",
    "X_train_clean = X_train.rename(columns=lambda col: clean_column(col))\n",
    "X_test_clean = X_test.rename(columns=lambda col: clean_column(col))\n",
    "\n",
    "\n",
    "# Обучение моделей\n",
    "logit = LogisticRegression(max_iter=1000)\n",
    "logit.fit(X_train_scaled, y_train)\n",
    "y_pred_logit = logit.predict(X_test_scaled)\n",
    "\n",
    "rf = RandomForestClassifier()\n",
    "rf.fit(X_train, y_train)\n",
    "y_pred_rf = rf.predict(X_test)\n",
    "\n",
    "xgb = XGBClassifier(use_label_encoder=False, eval_metric='logloss')\n",
    "xgb.fit(X_train, y_train)\n",
    "y_pred_xgb = xgb.predict(X_test)\n",
    "\n",
    "lgb = LGBMClassifier()\n",
    "lgb.fit(X_train_clean, y_train)\n",
    "y_pred_lgb = lgb.predict(X_test_clean)\n",
    "\n",
    "for name, preds, clf, xtest in [(\"Logistic Regression\", y_pred_logit, logit, X_test_scaled),\n",
    "                                (\"Random Forest\", y_pred_rf, rf, X_test),\n",
    "                                (\"XGBoost\", y_pred_xgb, xgb, X_test),\n",
    "                                (\"LightGBM\", y_pred_lgb, lgb, X_test)]:\n",
    "    print(f\"{name}: Accuracy={accuracy_score(y_test, preds):.3f}, Precision={precision_score(y_test, preds):.3f}, Recall={recall_score(y_test, preds):.3f}, F1={f1_score(y_test, preds):.3f}, AUC={roc_auc_score(y_test, clf.predict_proba(xtest)[:, 1]):.3f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cac36cb",
   "metadata": {},
   "source": [
    "1. Logistic Regression\n",
    "\n",
    "Accuracy = 0.711:\n",
    "Модель правильно классифицирует 71,1% объектов, что является наивысшим показателем среди представленных моделей.\n",
    "\n",
    "Precision = 0.600:\n",
    "Из всех объектов, предсказанных как положительные, 60,0% действительно положительные. Это говорит о некотором количестве ложноположительных случаев.\n",
    "\n",
    "Recall = 0.565:\n",
    "Модель обнаруживает 56,5% всех реальных положительных случаев, что означает неплохую, но не выдающуюся способность захватывать все нужные объекты.\n",
    "\n",
    "F1 = 0.582:\n",
    "Баланс между precision и recall держится на среднем уровне — примерно 58,2%.\n",
    "\n",
    "AUC = 0.703:\n",
    "Это значение свидетельствует об умеренной способности логистической регрессии различать классы при изменении порога.\n",
    "\n",
    "Интерпретация:\n",
    "Логистическая регрессия показывает наивысшую accuracy, что может свидетельствовать о том, что её решения при фиксированном пороге дают достаточно верные результаты. Однако относительная невысокая полнота (Recall) и AUC указывают, что при изменении порога модель не так хорошо различает положительные и отрицательные случаи.\n",
    "\n",
    "2. Random Forest\n",
    "\n",
    "Accuracy = 0.691:\n",
    "Общая точность чуть ниже — 69,1%, что немного уступает Logistic Regression.\n",
    "\n",
    "Precision = 0.579:\n",
    "Из предсказанных положительных объектов 57,9% являются истинно положительными, что немного хуже логистической регрессии.\n",
    "\n",
    "Recall = 0.478:\n",
    "Модель обнаруживает лишь 47,8% реальных положительных случаев — значение заметно ниже, что указывает на проблематику в находке всех позитивов.\n",
    "\n",
    "F1 = 0.524:\n",
    "Гармоническое среднее между precision и recall также ниже, чем у Logistic Regression, отражая общую потерю в балансе между точностью и полнотой.\n",
    "\n",
    "AUC = 0.731:\n",
    "Несмотря на несколько худшие показатели при фиксированном пороге, AUC выше (0.731). Это говорит о лучшей способности модели ранжировать объекты, даже если выбранный порог не оптимален.\n",
    "\n",
    "Интерпретация:\n",
    "Random Forest работает несколько осторожнее, что выражается в меньшей полноте (то есть модель с большей вероятностью пропускает реальные положительные примеры), но при этом способность различать классы (AUC) улучшена по сравнению с Logistic Regression. Это может указывать на потенциал модели при дополнительной настройке порогов или весов.\n",
    "\n",
    "3. XGBoost\n",
    "\n",
    "Accuracy = 0.696:\n",
    "Общая точность составляет 69,6%, что немного ниже показателя Logistic Regression, но сопоставимо с Random Forest.\n",
    "\n",
    "Precision = 0.566:\n",
    "Из предсказанных положительных только 56,6% являются истинными положительными, то есть модель склонна выдавать больше ложноположительных результатов.\n",
    "\n",
    "Recall = 0.623:\n",
    "При этом полнота равна 62,3% — самое высокое значение среди моделей, что свидетельствует о лучшем обнаружении всех позитивных случаев.\n",
    "\n",
    "F1 = 0.593:\n",
    "Баланс между precision и recall чуть улучшен (59,3%), что говорит о том, что модель работает с небольшим перекосом в сторону повышения обнаружения позитивов за счёт точности.\n",
    "\n",
    "AUC = 0.735:\n",
    "Это один из лучших показателей AUC, указывающий на высокую способность модели различать классы по вероятностной шкале.\n",
    "\n",
    "Интерпретация:\n",
    "XGBoost демонстрирует лучший recall, то есть успешно находит больше истинных положительных примеров, что может быть особенно актуально в задачах, где критично не пропустить позитивы. Несмотря на несколько более низкую precision, AUC остаётся высоким, что говорит о хорошем ранжировании объектов по принадлежности к классам. Таким образом, модель эффективна для задач, где важна полнота, даже если это идет в ущерб точности предсказаний.\n",
    "\n",
    "4. LightGBM\n",
    "\n",
    "Accuracy = 0.675:\n",
    "Общая точность равна 67,5%, что является самым низким значением среди рассмотренных моделей.\n",
    "\n",
    "Precision = 0.548:\n",
    "Из предсказанных положительных 54,8% действительно положительные – значение precision ниже, чем у остальных моделей.\n",
    "\n",
    "Recall = 0.493:\n",
    "Полнота составляет 49,3%, что сигнализирует о том, что модель пропускает более половины реальных положительных примеров.\n",
    "\n",
    "F1 = 0.519:\n",
    "Сбалансированный показатель между precision и recall также самый низкий — всего 51,9%.\n",
    "\n",
    "AUC = 0.736:\n",
    "Интересно, что AUC LightGBM оказывается самым высоким (или сопоставимым с лучшими) — 0.736, что говорит о том, что при варьировании порога модель способна хорошо разделять классы. Однако при фиксированном пороге ее предсказания оказываются менее точными.\n",
    "\n",
    "Интерпретация:\n",
    "Несмотря на лучший показатель AUC, оценка по фиксированному порогу (accuracy, precision, recall, F1) для LightGBM оказалась самой низкой. Это может означать, что оптимальный порог для принятия решения не был подобран, либо модель требует дополнительной калибровки. Высокий AUC указывает, что LightGBM имеет потенциал для различения классов, если правильно настроить пороговое значение.\n",
    "\n",
    "Итоговое сравнение и рекомендации\n",
    "\n",
    "Фиксированный порог vs. способность ранжирования:\n",
    "\n",
    "Logistic Regression показывает наивысшую accuracy (0.711) и хорошие сбалансированные показатели (precision = 0.600, recall = 0.565, F1 = 0.582), но имеет самый низкий AUC (0.703).\n",
    "Random Forest уступает Logistic Regression по accuracy, precision и recall, но выигрывает по AUC (0.731), что свидетельствует о потенциале модели при оптимальном выборе порога.\n",
    "XGBoost демонстрирует самый высокий recall (0.623) и один из лучших показателей AUC (0.735). Это может быть полезно, если задача требует обнаружения всех позитивных случаев, даже если точность предсказаний немного ниже.\n",
    "LightGBM показывает худшие значения по пороговым метрикам (accuracy, precision, recall, F1), но имеет самый высокий AUC (0.736). Это указывает на то, что LightGBM хорошо различает классы в ранжировании, однако оптимизация порога или калибровка вероятностей может повысить её практическую эффективность.\n",
    "Выбор модели зависит от задачи:\n",
    "Если требуется максимальная точность фиксированного порога, то Logistic Regression оказывается наиболее стабильной.\n",
    "Если важнее не пропустить позитивы, XGBoost с его высоким recall является предпочтительным выбором, несмотря на снижение precision.\n",
    "Если планируется дальнейшая настройка порога для повышения разделительной способности, модели с высоким AUC (Random Forest и LightGBM) могут дать преимущества после дополнительной оптимизации.\n",
    "\n",
    "Таким образом, каждая модель имеет свои сильные и слабые стороны. Итоговый выбор модели следует делать исходя из специфики задачи и требований: например, снижать вероятность пропуска важных случаев (Recall) или минимизировать ложноположительные предсказания (Precision), а также учитывать возможность дальнейшей настройки порогов, опираясь на значение AUC.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
